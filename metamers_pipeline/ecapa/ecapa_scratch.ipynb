{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/speechbrain/speechbrain.git@develop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/om2/user/salavill/misc/voice-speech-metamers/metamers_pipeline/robustness\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattack_steps\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m L2Step\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m helpers\n",
      "File \u001b[0;32m/om2/user/salavill/misc/voice-speech-metamers/metamers_pipeline/robustness/tools/helpers.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrobustness\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_transforms\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_attr\u001b[39m(obj, k):\n",
      "File \u001b[0;32m/om2/user/salavill/misc/voice-speech-metamers/metamers_pipeline/robustness/tools/constants.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcox\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# dog (117), cat (5), frog (3), turtle (5), bird (21), \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# monkey (14), fish (9), crab (4), insect (20) \u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cox'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "# from speechbrain.inference.speaker import SpeakerRecognition\n",
    "\n",
    "\n",
    "###UPDATE add robustness in inputs\n",
    "sys.path.append('/om2/user/salavill/misc/voice-speech-metamers/metamers_pipeline/robustness')\n",
    "from attack_steps import L2Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute your speaker embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes about input\n",
    "- needs to 16 kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = '/om2/user/amagaro/voice-speech-metamers/metamers_pipeline/kell2018/metamers/psychophysics_wsj400_jsintest_inversion_loss_layer_RS0_I3000_N8/0_SOUND_million/orig.wav'\n",
    "audio2 = '/om2/user/amagaro/voice-speech-metamers/metamers_pipeline/kell2018/metamers/psychophysics_wsj400_jsintest_inversion_loss_layer_RS0_I3000_N8/1_SOUND_would/orig.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EncoderClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get embedding for speaker\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderClassifier\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_hparams(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechbrain/spkrec-ecapa-voxceleb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# signal, fs = torchaudio.load(audio)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # make sure to resample to appropriate frequency\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# resampler = T.Resample(fs, 16000, dtype=signal.dtype)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# signal = resampler(signal)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# embeddings = model.encode_batch(signal)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EncoderClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Get embedding for speaker\n",
    "model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "# signal, fs = torchaudio.load(audio)\n",
    "# # make sure to resample to appropriate frequency\n",
    "# resampler = T.Resample(fs, 16000, dtype=signal.dtype)\n",
    "# signal = resampler(signal)\n",
    "# embeddings = model.encode_batch(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in sound\n",
    "signal, fs = torchaudio.load(audio)\n",
    "# make sure to resample to appropriate frequency\n",
    "resampler = T.Resample(fs, 16000, dtype=signal.dtype)\n",
    "signal = resampler(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = model.encode_batch(signal)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random noise\n",
    "NOISE_SCALE = .0000001\n",
    "im_n_initialized = ((torch.randn_like(signal)) * NOISE_SCALE ).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InversionLossLayer_ecapa(torch.nn.Module):\n",
    "    \"\"\"Loss used for most metamer generation experiments\"\"\"\n",
    "    def __init__(self, normalize_loss=False):\n",
    "        super(InversionLossLayer_ecapa, self).__init__()\n",
    "        self.normalize_loss = normalize_loss\n",
    "\n",
    "    def forward(self, model, inp, targ):\n",
    "        # CHange to get ecapa model embeddings\n",
    "        all_outputs = model.encode_batch(inp)\n",
    "        rep = all_outputs[0][0]\n",
    "        if self.normalize_loss:\n",
    "            loss = torch.div(torch.norm(rep - targ, dim=1), torch.norm(targ, dim=1))\n",
    "        else:\n",
    "            loss = torch.norm(rep - targ, dim=1)\n",
    "        return loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "normalize_loss = False ###UPDATE: idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_loss = InversionLossLayer_ecapa(normalize_loss = normalize_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through and update Noise\n",
    "\n",
    "- might need to constrain the update of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# synth_kwargs = {\n",
    "#     'custom_loss': custom_synthesis_losses.LOSSES[LOSS_FUNCTION](layer_to_invert, normalize_loss=True),\n",
    "#     'constraint':'2',\n",
    "#     'eps':100000,\n",
    "#     'step_size': step_size,\n",
    "#     'iterations': ITERATIONS,\n",
    "#     'do_tqdm': False,\n",
    "#     'targeted': True,\n",
    "#     'use_best': False\n",
    "# }\n",
    "eps = 100000\n",
    "step_size = 1\n",
    "\n",
    "\n",
    "random_start = 0 #UPDATE: IDK\n",
    "do_tqdm = 0 # import \n",
    "\n",
    "targeted = 1\n",
    "m = -1 if targeted else 1 #UPDATE: idk\n",
    "\n",
    "dataset_min_value = signal.min_value\n",
    "dataset_max_value = signal.max_value\n",
    "\n",
    "use_best = 0 #\n",
    "\n",
    "iterations = 100 # in the pipeline Janelle uses 3000\n",
    "return_image = 1\n",
    "\n",
    "est_grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iter n times\n",
    "# set noise as parameters to update\n",
    "\n",
    "# update these parameters \n",
    "step = L2Step(eps=eps, orig_input=signal, step_size=step_size,\n",
    "            min_value=dataset_min_value, max_value=dataset_max_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for making adversarial examples\n",
    "def get_adv_examples(x):\n",
    "    # Random start (to escape certain types of gradient masking)\n",
    "    if random_start:\n",
    "        x = step.random_perturb(x)\n",
    "\n",
    "    iterator = range(iterations)\n",
    "    if do_tqdm: iterator = tqdm(iterator)\n",
    "\n",
    "    # Keep track of the \"best\" (worst-case) loss and its\n",
    "    # corresponding input\n",
    "    best_loss = None\n",
    "    best_x = None\n",
    "\n",
    "    # A function that updates the best loss and best input\n",
    "    def replace_best(loss, bloss, x, bx):\n",
    "        if bloss is None:\n",
    "            bx = x.clone().detach()\n",
    "            bloss = loss.clone().detach()\n",
    "        else:\n",
    "            replace = m * bloss < m * loss\n",
    "            bx[replace] = x[replace].clone().detach()\n",
    "            bloss[replace] = loss[replace]\n",
    "\n",
    "        return bloss, bx\n",
    "\n",
    "    # PGD iterates\n",
    "    for _ in iterator:\n",
    "        x = x.clone().detach().requires_grad_(True)\n",
    "        losses, out = calc_loss(step.to_image(x), signal)\n",
    "        assert losses.shape[0] == x.shape[0], \\\n",
    "                'Shape of losses must match input!'\n",
    "\n",
    "        loss = torch.mean(losses)\n",
    "\n",
    "        if step.use_grad:\n",
    "            if est_grad is None:\n",
    "                grad, = torch.autograd.grad(m * loss, [x])\n",
    "            # else:\n",
    "            #     f = lambda _x, _y: m * calc_loss(step.to_image(_x), _y)[0]\n",
    "            #     grad = helpers.calc_est_grad(f, x, target, *est_grad)\n",
    "        else:\n",
    "            grad = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            args = [losses, best_loss, x, best_x]\n",
    "            best_loss, best_x = replace_best(*args) if use_best else (losses, x)\n",
    "\n",
    "            x = step.step(x, grad)\n",
    "            x = step.project(x)\n",
    "            if do_tqdm: iterator.set_description(\"Current loss: {l}\".format(l=loss))\n",
    "\n",
    "    # Save computation (don't compute last loss) if not use_best\n",
    "    if not use_best: \n",
    "        ret = x.clone().detach()\n",
    "        return step.to_image(ret) if return_image else ret\n",
    "\n",
    "    losses, _ = calc_loss(step.to_image(x), target)\n",
    "    args = [losses, best_loss, x, best_x]\n",
    "    best_loss, best_x = replace_best(*args)\n",
    "    return step.to_image(best_x) if return_image else best_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xadv = get_adv_examples(im_n_initialized)\n",
    "this_loss = calc_loss(model, xadv, target.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "\n",
    "    im_n = xadv\n",
    "    # step.step_size = step.step_size/2\n",
    "    # get metamer\n",
    "    xadv = get_adv_examples(im_n)\n",
    "    # calculate new loss\n",
    "    this_loss = calc_loss(model, xadv, target.clone())\n",
    "    if i%10 == 0:\n",
    "        print(f'Iteration: {i}, Loss: {this_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play metamer\n",
    "xadv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Perform Speaker Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for speaker\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "signal, fs = torchaudio.load(audio)\n",
    "# make sure to resample to appropriate frequency\n",
    "resampler = T.Resample(fs, 16000, dtype=signal.dtype)\n",
    "signal = resampler(signal)\n",
    "embeddings = classifier.encode_batch(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([True])\n"
     ]
    }
   ],
   "source": [
    "# verify if 2 speakers are the same\n",
    "verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n",
    "# score, prediction = verification.verify_files(audio, audio) # Same Speaker\n",
    "# print(score, prediction)\n",
    "\n",
    "score, prediction = verification.verify_files(audio, audio2) # Different Speakers\n",
    "print(score, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,\n",
       " array([ 0.00759776,  0.00406835, -0.00337481, ..., -0.01439899,\n",
       "        -0.0056413 ,  0.01752515], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.io.wavfile.read(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 13.6807, -21.5077,  16.5698,  27.3773,  20.9341,  -2.6867,  47.9093,\n",
       "           10.9244,  43.0350, -29.4488, -50.7251,  23.6661,  36.2392,   5.4932,\n",
       "            4.8596, -14.1890,  -7.8909,   3.0897,  40.3050, -14.0334, -18.2770,\n",
       "          -19.1589,  33.1250, -16.0443, -15.0739,  -6.6625, -21.3170,  -4.1494,\n",
       "           14.0383, -15.5008, -30.3342,  41.4378,  25.2998,   7.5653,  15.8887,\n",
       "           10.5355, -24.6812, -25.7595, -19.5165,   2.4563,  23.4598,  12.1603,\n",
       "          -14.5642,  -8.9963,   8.4244, -45.2159,  23.8665,  12.0209,  -9.0004,\n",
       "           29.5747,  15.1678,  17.9769, -12.2870,   0.7903,  10.2497,  11.3309,\n",
       "           -9.3533,  -0.3147,   7.0483, -10.6227,   5.6716, -13.1225, -24.9788,\n",
       "            6.1509,  48.9741, -20.4996,  -0.8317, -11.3773,  27.6694, -15.0420,\n",
       "            8.6980, -20.4009,  15.0508,  13.3836, -20.7181,  -4.8599, -22.3480,\n",
       "          -35.1690, -11.5540,  14.0490, -29.5636,  29.2453,  21.0988,  40.4526,\n",
       "           50.2919, -13.8887, -21.6431,  -9.9248,  -9.1962,  -2.9276,  -7.0444,\n",
       "          -38.1359,  39.2166,  37.8656,  -9.5143,  37.6102,  27.1740, -19.6869,\n",
       "           -3.0432,  14.4827, -22.1826, -12.8738,  -2.0284, -33.0292,  37.6878,\n",
       "           -4.1000, -32.6151, -19.4412,  31.0585, -14.5880,  20.8503,  -4.7765,\n",
       "          -25.2200,   3.1581,  -6.6610,  33.7479,  37.1570, -20.8982,   1.6824,\n",
       "           -4.0898,  25.4413,  19.2584,  46.2135, -12.0263,  19.2132,  -1.1828,\n",
       "          -16.4516, -29.2477, -23.4088,   9.3969,   7.3238,  17.0340, -12.7715,\n",
       "          -40.8065,  11.1995,  -7.3251,  12.2980, -10.5231,   1.2568,  21.3036,\n",
       "           -9.7688,  -2.0788,  -3.6515,  -7.0545,  11.6758,  -0.4620, -32.1179,\n",
       "           13.2240,   2.4585, -50.8020,  16.5393, -37.4549,  18.3970, -14.2477,\n",
       "           -8.7477, -21.4266, -15.9857,   4.4624,  32.9570, -47.7712, -32.1362,\n",
       "           -2.3840,  37.3189,  -8.4452,  -0.9736,  34.4343, -38.1971, -12.8870,\n",
       "          -10.9011,  29.8694,  34.0624, -23.7648,  10.3747,   0.2858, -23.0930,\n",
       "          -23.8754,   0.8583,   0.4052, -27.4723,  13.9406,  -5.0013, -36.0990,\n",
       "           16.2367, -32.9408,  13.4850,   8.6084, -13.1577, -17.3568,   1.4543,\n",
       "          -11.6852,  18.8209,  -2.9398]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mods']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (blocks): ModuleList()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier._modules['mods']['classifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1256e+01, -4.4376e+00,  5.4710e+00, -4.4777e-01,  9.3813e+00,\n",
       "          -1.7214e+01,  2.0267e+01, -6.4316e+00,  1.1311e+00, -6.9549e+00,\n",
       "          -5.6514e+01,  1.5606e+01,  7.6293e+00,  9.4477e+00,  5.0318e+00,\n",
       "           6.7749e+00,  2.2657e+01,  6.6792e+00,  1.2737e+01,  4.7252e+00,\n",
       "          -1.1207e+00, -3.8659e+01,  5.9548e+01,  1.3946e+01, -2.6766e+01,\n",
       "          -1.5729e+01, -6.6083e+00, -2.0886e+01, -3.2234e+01, -3.1146e+01,\n",
       "           5.3255e+00,  1.0481e+01,  2.7337e+01,  2.0878e+01,  1.0505e+01,\n",
       "          -5.3502e+00,  1.4651e+01, -3.3391e+00, -1.6250e+00, -3.7518e+01,\n",
       "           3.7116e+01,  2.6891e+01, -3.3418e+01,  1.3859e+01,  7.2535e+00,\n",
       "          -3.0828e+00, -1.5202e+01,  8.6623e+00,  2.5144e+01,  3.2541e+01,\n",
       "           1.8105e+01,  4.8451e+00, -1.4034e+01,  3.1070e+01,  8.0962e+01,\n",
       "          -3.7654e+00, -3.5467e+01,  1.6404e+00, -4.7164e+01,  4.4524e+00,\n",
       "          -4.2345e+00, -1.4514e+01,  6.2556e-02,  4.1281e+00,  4.3622e+01,\n",
       "           1.6985e+01, -1.6516e+01, -2.6540e+01,  2.6727e+00, -3.1603e+01,\n",
       "           3.3258e+00, -3.5505e+01,  1.8720e+01, -2.1808e+01, -1.8872e+00,\n",
       "          -1.3916e+00, -2.7441e+01, -2.4367e+01, -5.7156e+00,  3.8040e+01,\n",
       "          -1.5415e+01, -3.0538e+01, -2.0303e+01,  6.6354e+01,  1.8730e+00,\n",
       "           8.0682e+00, -3.3328e+01,  1.3124e+01, -1.2918e+01, -1.7146e+01,\n",
       "          -2.3829e+01, -2.1727e+01, -6.1440e+00, -2.7612e+01, -1.6384e+01,\n",
       "           2.4526e+01, -6.5893e+00, -3.2722e+01, -1.9887e+01,  2.0634e+01,\n",
       "           1.7477e+01, -2.0259e+01,  3.7455e+01,  9.9712e+00, -1.8061e+01,\n",
       "          -1.9119e+01, -2.7633e+01, -3.4407e+01, -2.3868e+01,  2.3601e+00,\n",
       "           3.3531e-01,  1.3801e+00,  7.2202e+00,  1.6068e+00,  1.3931e+01,\n",
       "           2.4974e+01,  3.9576e+00, -1.2300e+00,  2.1811e+01, -2.1508e+01,\n",
       "           3.4360e+00,  5.9893e+01,  1.8813e+01,  2.9763e+01,  6.5974e+01,\n",
       "          -7.8075e+00, -2.5831e+01,  2.4590e+00, -5.1430e+00,  6.1580e+00,\n",
       "          -9.1837e+00, -8.3032e+00,  3.6710e+00,  2.1130e+01, -1.8913e+01,\n",
       "           7.3471e+00,  3.2700e+01, -1.4631e+01, -1.1275e+01, -1.5276e+01,\n",
       "           1.6434e+01, -4.3338e+01, -1.5480e+01, -1.7518e+01,  2.6551e+01,\n",
       "          -5.4128e+00,  1.0347e+01, -7.9452e+00,  3.4692e+01, -2.6122e+01,\n",
       "          -5.2684e+00,  2.8215e+01,  3.3654e+01,  1.3123e+01,  4.1015e+00,\n",
       "           5.6062e-01, -1.6394e+00, -3.1839e+00,  1.6575e+01, -4.9795e+01,\n",
       "           2.0771e+01,  5.3518e+01,  5.2620e+00,  4.5713e+00, -1.6673e+00,\n",
       "           2.9667e+00, -1.2669e+00, -2.0815e+01,  1.5269e+01, -1.7391e+01,\n",
       "          -3.3846e+00, -2.9206e+01,  8.5009e+00, -9.9963e+00,  3.6224e+01,\n",
       "          -2.4530e+01, -2.8037e+01, -7.1087e+00, -8.4272e+00,  4.8270e+00,\n",
       "           2.1113e+01, -4.2163e+01,  1.2716e+01, -4.7814e+00,  1.7438e+01,\n",
       "           3.1207e+00, -2.3804e+00, -4.5262e+01, -1.1057e+01,  6.8855e+00,\n",
       "          -1.8907e+00, -3.5303e+01]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([True]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
